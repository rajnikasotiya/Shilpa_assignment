import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from concurrent.futures import ThreadPoolExecutor
import math

# ====== CONFIG ======
model_name = "microsoft/Phi-2"
num_workers = 4   # adjust for your CPU cores
input_csv = "your_file.csv"
output_csv = "output_with_phi2.csv"

# ====== LOAD MODEL ======
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cpu",
    torch_dtype="auto"
)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=-1)

# ====== PROMPT FUNCTION (optimized) ======
def extract_medical_info(text):
    prompt = f"""
    Extract from the medical note in JSON:
    - visit_reason
    - diagnosis
    - past_medical_history
    - vital_signs
    - exam_results
    - lab_results
    - full_HPI
    - discharge_plan

    Medical Note:
    {text}
    """
    try:
        return pipe(prompt, max_length=512, truncation=True)[0]['generated_text']
    except Exception as e:
        return f"ERROR: {e}"

# ====== PARALLEL PROCESSING ======
def process_chunk(chunk_df):
    chunk_df[model_name] = chunk_df['text'].apply(extract_medical_info)
    return chunk_df

# Read dataset
df = pd.read_csv(input_csv)

# Split into chunks for threads
chunk_size = math.ceil(len(df) / num_workers)
chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]

# Process in parallel with incremental saving
processed_chunks = []
with ThreadPoolExecutor(max_workers=num_workers) as executor:
    for processed_chunk in executor.map(process_chunk, chunks):
        processed_chunks.append(processed_chunk)
        pd.concat(processed_chunks).to_csv(output_csv, index=False)

# Final save
final_df = pd.concat(processed_chunks).reset_index(drop=True)
final_df.to_csv(output_csv, index=False)
print(f"âœ… Processing complete. Saved to {output_csv}")

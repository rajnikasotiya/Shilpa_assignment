import pandas as pd
import duckdb
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ==========================
# 1. Load CSV file
# ==========================
CSV_PATH = "COC_pharmacy_claims_sample.csv"   # your file
df = pd.read_csv(CSV_PATH)

print("CSV Loaded. Shape:", df.shape)
print("Columns:", df.columns.tolist()[:10], "...")  # preview

# ==========================
# 2. Load Text-to-SQL Model
# ==========================
MODEL_NAME = "gaussalgo/T5-LM-Large-text2sql-spider"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

# ==========================
# 3. Text-to-SQL Function with Few-Shot Prompting
# ==========================
def text_to_sql(question: str, schema: str = "") -> str:
    """
    Converts a natural language question into an SQL query using few-shot examples.
    """
    few_shot_examples = f"""
You are a SQL expert. 
Only use the table: claims_data
Columns available: {', '.join(df.columns.tolist())}

Important rules:
- Do NOT invent new tables (only use claims_data).
- Do NOT invent new columns.
- If the query involves totals or averages, compute them directly from claims_data.
- Always SELECT from claims_data.

### Examples
Question: Show the top 5 most expensive drugs.
SQL: SELECT LBL_NM, AVG(ANLYTC_PAID_AMT) AS avg_cost
     FROM claims_data
     GROUP BY LBL_NM
     ORDER BY avg_cost DESC
     LIMIT 5;

Question: Find members who spent more than 10000 in total.
SQL: SELECT MBR_KEY, SUM(ANLYTC_PAID_AMT) AS total_spent
     FROM claims_data
     GROUP BY MBR_KEY
     HAVING SUM(ANLYTC_PAID_AMT) > 10000;

Question: Members who refilled early and spent more than 10000 in total.
SQL: SELECT MBR_KEY, SUM(ANLYTC_PAID_AMT) AS total_spent
     FROM claims_data
     WHERE REFIL_IND = 1
     GROUP BY MBR_KEY
     HAVING SUM(ANLYTC_PAID_AMT) > 10000;

### Now generate:
Question: {question}
SQL:
"""
    inputs = tokenizer.encode(few_shot_examples, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs, max_length=256, num_beams=4, early_stopping=True)
    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # âœ… Hard fix: ensure table name is claims_data
    if "FROM" in sql_query and "claims_data" not in sql_query:
        sql_query = sql_query.replace("FROM", "FROM claims_data")

    return sql_query

# ==========================
# 4. Interactive Loop
# ==========================
duckdb.register("claims_data", df)

print("\nAsk me anything about your data! (type 'exit' to quit)\n")

while True:
    question = input("? Enter your question: ")
    if question.lower() in ["exit", "quit", "q"]:
        print("Exiting. Goodbye!")
        break

    # Generate SQL
    sql_query = text_to_sql(question)
    print("\nGenerated SQL:\n", sql_query)

    # Run SQL safely
    try:
        result = duckdb.query(sql_query).to_df()
        print("\nQuery Result:")
        print(result.head(10))  # show top 10 rows
    except Exception as e:
        print("\nSQL Execution Failed:", e)

    print("\n" + "="*60 + "\n")

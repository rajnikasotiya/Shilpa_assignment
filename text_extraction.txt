import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import math

# ==== SETTINGS ====
input_file = "results.csv"   # Your input file
text_column = "context"      # Column containing input text
model_name = "microsoft/Phi-2"
output_column = model_name.replace("/", "_")  # Column name for results
batch_size = 10              # Process rows in batches
max_length = 400             # Reduce output size for faster processing

# ==== LOAD DATA ====
df = pd.read_csv(input_file)

# ==== LOAD MODEL/TOKENIZER (CPU) ====
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)  # CPU mode
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1  # CPU only
)

# ==== DEFINE PROMPT ====
def build_prompt(text):
    return f"""
    Extract the following from the medical note in JSON format:
    - visit_reason
    - diagnosis
    - past_medical_history
    - vital_signs
    - exam_results
    - lab_results
    - full_HPI
    - discharge_plan
    
    Medical Note:
    {text}
    """

# ==== PROCESS IN BATCHES ====
results = []
num_batches = math.ceil(len(df) / batch_size)

for i in range(num_batches):
    batch_texts = df[text_column].iloc[i*batch_size:(i+1)*batch_size].tolist()
    prompts = [build_prompt(t) for t in batch_texts]
    outputs = pipe(prompts, max_length=max_length, truncation=True)
    batch_results = [o[0]['generated_text'] for o in outputs]
    results.extend(batch_results)
    print(f"Processed batch {i+1}/{num_batches}")

# ==== ADD NEW COLUMN ====
df[output_column] = results

# ==== SAVE BACK TO SAME FILE ====
df.to_csv(input_file, index=False)
print(f"âœ… Updated '{input_file}' with new column '{output_column}'")

!pip install --upgrade transformers accelerate safetensors pandas --quiet

import pandas as pd
import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from concurrent.futures import ProcessPoolExecutor
import os

# -------------------
# CONFIG
# -------------------
model_name = "microsoft/Phi-2"
output_col = model_name.replace("/", "_")  # => "microsoft_Phi_2"
max_new_tokens = 200
temperature = 0.0
num_workers = os.cpu_count() // 2  # use half of available cores to avoid RAM pressure

# -------------------
# PROMPT TEMPLATE
# -------------------
def build_prompt(text):
    return f"""
Extract the following from the medical note in valid JSON format:
- visit_reason
- diagnosis
- past_medical_history
- vital_signs
- exam_results
- lab_results
- full_HPI
- discharge_plan

Return ONLY JSON, no extra text.

Medical Note:
{text}
"""

# -------------------
# WORKER FUNCTION
# -------------------
def process_chunk(df_chunk):
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="cpu",
        torch_dtype=torch.float32,
        trust_remote_code=True
    )

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=-1,  # CPU
        batch_size=4
    )

    prompts = [build_prompt(t) for t in df_chunk["context"].tolist()]
    outputs = pipe(prompts, max_new_tokens=max_new_tokens, temperature=temperature, do_sample=False)

    results = []
    for out in outputs:
        resp = out["generated_text"]
        match = re.search(r"\{.*\}", resp, re.DOTALL)
        if match:
            try:
                parsed = json.loads(match.group(0))
                combined = " | ".join([f"{k}: {v}" for k, v in parsed.items()])
                results.append(combined)
                continue
            except json.JSONDecodeError:
                pass
        results.append("")  # fallback if parsing fails
    df_chunk[output_col] = results
    return df_chunk

# -------------------
# MAIN EXECUTION
# -------------------
if __name__ == "__main__":
    df = pd.read_csv("result.csv")

    # Split into chunks for parallel processing
    chunks = []
    chunk_size = len(df) // num_workers + 1
    for i in range(0, len(df), chunk_size):
        chunks.append(df.iloc[i:i+chunk_size].copy())

    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        processed_chunks = list(executor.map(process_chunk, chunks))

    final_df = pd.concat(processed_chunks, ignore_index=True)
    final_df.to_csv("result_with_extractions.csv", index=False)

    print(f"âœ… Done! Saved to 'result_with_extractions.csv' with column '{output_col}'.")

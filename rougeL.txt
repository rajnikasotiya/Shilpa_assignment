import pandas as pd
import re
import json
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
from evaluate import load

# === Load CSV ===
df = pd.read_csv("results3.csv")
records = df["context"].head(10).tolist()
gpt4_refs = df["gpt4"].head(10).tolist()  # GPT-4 column

# === Field Definitions ===
FIELD_DEFINITIONS = {
    "Visit Reason": "Main complaint or reason why the patient came to the hospital.",
    "History": "Past medical history, chronic conditions, or relevant background information.",
    "Diagnosis": "The doctor's assessment or suspected/confirmed disease.",
    "Clinical Details": "Signs, symptoms, and clinical observations noted during examination.",
    "Lab Results": "Blood tests or laboratory values with findings.",
    "Treatment Plan": "Medications, therapies, or planned interventions.",
    "Vital Signs": "Objective measurements such as blood pressure, pulse, temperature, SpO2.",
    "Exam Results": "Findings from physical examination.",
    "Imaging Test Results": "Findings from imaging such as X-ray, ECG, MRI, CT.",
    "Hospital Course": "Description of what happened during hospitalization or stay.",
    "Discharge Plan": "Follow-up instructions, discharge medications, and next steps.",
    "Full HPI": "Narrative description of the patient's history of present illness.",
    "Full Assessment and Plan": "Complete summary of assessment and treatment plan."
}

FIELDS = list(FIELD_DEFINITIONS.keys())
NARRATIVE_FIELDS = {"Full HPI", "Full Assessment and Plan"}

# === Load Models ===
extractor = pipeline("text2text-generation", model="google/flan-t5-base")
embedder = SentenceTransformer("all-MiniLM-L6-v2")
rouge = load("rouge")

results = []

# === Process Each Record ===
for record_text in records:
    # === Prompt for extraction ===
    prompt = (
        "Extract the following fields from the medical record and return in JSON format. "
        "If a field is not explicitly mentioned, extract the most relevant and non-repeated phrase from the text, or leave it empty.\n"
        "Return each field as a list of short phrases.\n\n"
        "Fields to extract with definitions:\n"
    )
    for field, definition in FIELD_DEFINITIONS.items():
        prompt += f"{field}: {definition}\n"
    prompt += f"\nMedical Record:\n{record_text}"

    # === Run LLM ===
    response = extractor(prompt, max_length=1536, clean_up_tokenization_spaces=True)[0]["generated_text"]

    # === Try Parsing JSON ===
    try:
        structured_data = json.loads(response)
        structured_data = {k: v if isinstance(v, list) else [v] for k, v in structured_data.items()}
    except:
        structured_data = {field: [] for field in FIELDS}

    # === Prepare sentences for fallback ===
    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', record_text) if s.strip()]
    sentence_embeddings = embedder.encode(sentences, convert_to_tensor=True)

    # === Fallback using Cosine Similarity ===
    for field in FIELDS:
        value = structured_data.get(field, [])
        if not value:
            if sentences:
                query_text = FIELD_DEFINITIONS[field]
                field_embedding = embedder.encode(query_text, convert_to_tensor=True)
                similarity_scores = util.cos_sim(field_embedding, sentence_embeddings)[0]
                best_idx = int(similarity_scores.argmax())
                best_score = float(similarity_scores[best_idx])

                if best_score > 0.35:
                    fallback_text = sentences[best_idx]
                    if field not in NARRATIVE_FIELDS:
                        fallback_text = " ".join([w for w in fallback_text.split() if len(w) > 2][:6])
                    structured_data[field] = [fallback_text.strip()]
                else:
                    structured_data[field] = []
            else:
                structured_data[field] = []

    results.append(structured_data)

# === Field-wise Evaluation ===
final_scores = {field: {"rouge": [], "cosine": [], "exact_match": []} for field in FIELDS}

for pred, ref_json in zip(results, gpt4_refs):
    try:
        reference_data = json.loads(ref_json)
    except:
        reference_data = {field: [] for field in FIELDS}

    for field in FIELDS:
        pred_text = " ".join(pred.get(field, []))
        ref_text = " ".join(reference_data.get(field, []))

        # ROUGE-L
        if ref_text and pred_text:
            rouge_score = rouge.compute(predictions=[pred_text], references=[ref_text])["rougeL"]
        else:
            rouge_score = 0.0

        # Cosine Similarity
        if ref_text and pred_text:
            emb_pred = embedder.encode(pred_text, convert_to_tensor=True)
            emb_ref = embedder.encode(ref_text, convert_to_tensor=True)
            cosine_sim = float(util.cos_sim(emb_pred, emb_ref))
        else:
            cosine_sim = 0.0

        # Exact Match
        exact_match = 1.0 if pred_text.lower() == ref_text.lower() and pred_text != "" else 0.0

        final_scores[field]["rouge"].append(rouge_score)
        final_scores[field]["cosine"].append(cosine_sim)
        final_scores[field]["exact_match"].append(exact_match)

# === Average Scores per Field ===
for field in FIELDS:
    rouge_avg = sum(final_scores[field]["rouge"]) / len(final_scores[field]["rouge"])
    cosine_avg = sum(final_scores[field]["cosine"]) / len(final_scores[field]["cosine"])
    exact_avg = sum(final_scores[field]["exact_match"]) / len(final_scores[field]["exact_match"])
    print(f"\nField: {field}")
    print(f"  ROUGE-L: {rouge_avg:.3f}")
    print(f"  Cosine Similarity: {cosine_avg:.3f}")
    print(f"  Exact Match: {exact_avg:.3f}")

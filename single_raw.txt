import pandas as pd
import json
import re
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# === Load CSV ===
df = pd.read_csv("results3.csv")
record_text = str(df.loc[0, "context"])  # first record only

# === Define fields to extract ===
FIELDS = [
    "Visit Reason",
    "History",
    "Diagnosis",
    "Lab Results",
    "Treatment Plan",
    "Clinical Details",
    "Vital Signs",
    "Exam Results",
    "Imaging Test Results",
    "Hospital and ED course",
    "Discharge Plan",
    "Full HPI",
    "Full Assessment and Plan"
]

# === Load models ===
extractor = pipeline("text2text-generation", model="google/flan-t5-base")
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# === Stopwords for cleaning ===
stop_words = set(stopwords.words("english"))

def clean_keywords(text):
    """Extract short medical keywords from a sentence."""
    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t.isalpha() and t.lower() not in stop_words]
    # Keep 1â€“3 word chunks
    keywords = []
    for i in range(len(tokens)):
        # single word
        keywords.append(tokens[i])
        # two-word phrase
        if i < len(tokens) - 1:
            keywords.append(tokens[i] + " " + tokens[i+1])
        # three-word phrase
        if i < len(tokens) - 2:
            keywords.append(tokens[i] + " " + tokens[i+1] + " " + tokens[i+2])
    # Deduplicate and keep medical-looking words
    keywords = list(dict.fromkeys([k.lower() for k in keywords if len(k) > 2]))
    return keywords[:5]  # limit per field

# === Prompt for structured extraction ===
prompt = f"""
Extract the following fields from the medical record and return in JSON format.
Return each field as a list of the most relevant keywords or short medical terms (not full sentences).
For example, use terms like 'chest pain', 'shortness of breath', 'hypertension', 'diabetes', 
'ST-T changes', 'metformin 500mg' etc. Avoid generic words.

Fields to extract:
{", ".join(FIELDS)}

Medical Record:
{record_text}
"""

# === Run model ===
response = extractor(prompt, max_length=1024, clean_up_tokenization_spaces=True)[0]['generated_text']

# === Try to parse JSON ===
try:
    structured_data = json.loads(response)
except:
    structured_data = {field: [] for field in FIELDS}  # fallback empty dict

# === Cosine similarity fallback for missing fields ===
sentences = record_text.split(". ")
sentence_embeddings = embedder.encode(sentences, convert_to_tensor=True)

for field in FIELDS:
    value = structured_data.get(field, [])
    if not value or value == "":
        field_embedding = embedder.encode(field, convert_to_tensor=True)
        similarity_scores = util.cos_sim(field_embedding, sentence_embeddings)[0]
        best_idx = int(similarity_scores.argmax())
        best_sentence = sentences[best_idx].strip()
        structured_data[field] = clean_keywords(best_sentence)

# === Final Output ===
print(json.dumps(structured_data, indent=2))

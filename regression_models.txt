# Insurance Regression Model - Full Pipeline

# ✅ STEP 1: Import Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_log_error, make_scorer

# ✅ STEP 2: Load Data
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# ✅ STEP 3: Quick EDA
print(train.info())
print(train.describe())
print(train.isna().sum())
sns.histplot(train['target'], kde=True)
plt.title('Distribution of Target')
plt.show()

# ✅ STEP 4: Feature Engineering
def add_features(df):
    df['log_income'] = np.log1p(df['Annual_Income'])
    df['claims_per_year'] = df['Previous_Claims'] / (df['Policy_Years'] + 1)
    df['age_income_interaction'] = df['Age'] * df['log_income']
    return df

train = add_features(train)
test = add_features(test)

# ✅ STEP 5: Preprocessing and Feature Selection
X = train.drop(['id','target'], axis=1)
y = train['target']

preprocessor = ColumnTransformer([
    ('num', StandardScaler(), ['Age','Annual_Income','Previous_Claims','Policy_Years','claims_per_year','age_income_interaction']),
    ('cat', OneHotEncoder(drop='first'), ['Gender','Region','Vehicle_Type'])
])

sel_pipe = Pipeline([
    ('pre', preprocessor),
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42))
]).fit(X, y)

importances = sel_pipe.named_steps['rf'].feature_importances_
features = preprocessor.get_feature_names_out().tolist()
feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)
top_feats = feat_imp.head(20).index.tolist()
print("Top Features:\n", top_feats)

# ✅ STEP 6: Define Models and Parameter Grids
models = {
 'XGBoost': (XGBRegressor(random_state=42), {
    'n_estimators': [100,200],
    'max_depth': [3,5],
    'learning_rate': [0.05,0.1]
 }),
 'Random Forest': (RandomForestRegressor(random_state=42), {
    'n_estimators': [100,200],
    'max_depth': [None,10],
    'min_samples_split': [2,5]
 }),
 'Gradient Boosting': (GradientBoostingRegressor(random_state=42), {
    'n_estimators': [100,200],
    'max_depth': [3,5],
    'learning_rate': [0.05,0.1]
 })
}

# ✅ STEP 7: Train and Tune with GridSearchCV
cv = KFold(n_splits=5, shuffle=True, random_state=42)
msle = make_scorer(mean_squared_log_error, greater_is_better=False)

results = []
for name, (model, params) in models.items():
    pipe = Pipeline([('pre', preprocessor), ('model', model)])
    gs = GridSearchCV(pipe, param_grid=params, cv=cv, scoring=msle, n_jobs=-1, verbose=1)
    gs.fit(X, y)
    score = -gs.best_score_
    print(f"{name}: RMSLE = {np.sqrt(score):.5f}")
    results.append((name, gs))

# ✅ STEP 8: Select Best Model
best_name, best_gs = min(results, key=lambda x: -x[1].best_score_)
print(f"Best model: {best_name}, RMSLE: {np.sqrt(-best_gs.best_score_):.5f}")

# ✅ STEP 9: Predict on Test Data
test_preds = best_gs.predict(test.drop(['id'], axis=1))
submission = pd.DataFrame({'id': test['id'], 'target': test_preds})
submission.to_csv('submission.csv', index=False)
print("Submission saved as submission.csv")

# Step 0: Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor

# Step 1: Load the Data
df = pd.read_csv("insurance_dataset.csv")  # Replace with your CSV path

# Drop ID or first index column
df = df.drop(columns=[df.columns[0]])

# Step 2: EDA Plots
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Step 3: Feature Engineering
df['IncomePerChild'] = df['AnnualInc'] / (df['Number of Children'] + 1)
df['ClaimsPerVehicleAge'] = df['Previous Claims'] / (df['Vehicle Age'] + 1)

# Drop policy start time
df.drop(columns=['Policy Start Time'], inplace=True)

# Binary mapping
df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})
df['Smoking Status'] = df['Smoking Status'].map({'No': 0, 'Yes': 1})

# Define target
target = 'Premium Amount'
y = df[target]
X = df.drop(columns=[target])

# Categorical & numeric columns
categorical_cols = X.select_dtypes(include='object').columns.tolist()
numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Preprocessor
preprocessor = ColumnTransformer([
    ('onehot', OneHotEncoder(drop='first'), categorical_cols),
    ('scale', StandardScaler(), numeric_cols)
])

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Models to Evaluate
models = {
    "LinearRegression": LinearRegression(),
    "Ridge": Ridge(),
    "Lasso": Lasso(),
    "RandomForest": RandomForestRegressor(),
    "GradientBoosting": GradientBoostingRegressor(),
    "XGBoost": XGBRegressor(),
    "SVR": SVR()
}

results = {}
predictions = {}

for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])
    pipeline.fit(X_train, y_train)
    preds = pipeline.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, preds))
    mae = mean_absolute_error(y_test, preds)
    r2 = r2_score(y_test, preds)
    results[name] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}
    predictions[name] = preds

# Performance Comparison
results_df = pd.DataFrame(results).T.sort_values(by='RMSE')
print("ðŸ“Š Model Comparison:\n", results_df)

# Plot Residuals for Best Model (e.g., XGBoost)
best_model_name = results_df.index[0]
best_model = models[best_model_name]
pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', best_model)])
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

residuals = y_test - y_pred
plt.figure(figsize=(8, 5))
sns.histplot(residuals, bins=30, kde=True)
plt.title(f'Residual Distribution for {best_model_name}')
plt.xlabel("Residual")
plt.ylabel("Frequency")
plt.show()

# Step 8: Hyperparameter Tuning (XGBoost)
param_grid = {
    'regressor__n_estimators': [50, 100, 200],
    'regressor__max_depth': [3, 5, 7],
    'regressor__learning_rate': [0.01, 0.1, 0.2]
}

xgb_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(random_state=42))
])

grid = GridSearchCV(xgb_pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)
grid.fit(X_train, y_train)

print("\nðŸ”§ Best XGBoost Parameters:", grid.best_params_)

# Final Evaluation
final_model = grid.best_estimator_
final_preds = final_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, final_preds))
r2 = r2_score(y_test, final_preds)

print(f"\nâœ… Final Model (XGBoost) Performance:\nRMSE = {rmse:.2f}, RÂ² = {r2:.4f}")

# Plot Feature Importances
xgb = grid.best_estimator_['regressor']
ohe_columns = final_model['preprocessor'].named_transformers_['onehot'].get_feature_names_out(categorical_cols)
all_features = np.concatenate([ohe_columns, numeric_cols])
importances = xgb.feature_importances_

feat_imp_df = pd.DataFrame({
    'Feature': all_features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_imp_df.head(15))
plt.title("Top 15 Feature Importances (XGBoost)")
plt.tight_layout()
plt.show()

import pandas as pd
import re
from sentence_transformers import SentenceTransformer, util
import numpy as np

# === Load CSV ===
df = pd.read_csv("results.csv")

# === Categories & aliases ===
CATEGORY_ALIASES = {
    "Visit Reason": ["Visit Reason", "Reason for Visit", "Chief Complaint"],
    "Diagnosis": ["Diagnosis", "Final Diagnosis", "Provisional Diagnosis"],
    "Past Medical History": ["Past Medical History", "PMH", "History of Illness"],
    "Clinical Details": ["Clinical Details", "Clinical Notes", "Case Details"],
    "Vital Signs": ["Vital Signs", "Vitals", "Patient Vitals"],
    "Exam Results": ["Exam Results", "Physical Examination", "Exam Findings"],
    "Lab Results": ["Lab Results", "Laboratory Findings", "Labs"],
    "Imaging Test Results": ["Imaging Test Results", "Radiology", "Imaging Results"],
    "Hospital and ED course": ["Hospital and ED course", "ED Course", "Hospital Course"],
    "Discharge Plan": ["Discharge Plan", "Plan at Discharge", "Discharge Instructions"],
    "Full HPI": ["Full HPI", "History of Present Illness", "HPI"],
    "Full Assessment and Plan": ["Full Assessment and Plan", "Assessment and Plan", "Plan"]
}

CATEGORIES = list(CATEGORY_ALIASES.keys())

# === Load embedding model ===
embedder = SentenceTransformer("all-MiniLM-L6-v2")


def extract_with_best_similarity(text, threshold=0.65, top_k=3, extra_sentences=2):
    """
    Extract info for each category with heading match (fuzzy via aliases) 
    and fallback to top-N cosine similarity matches.
    Also appends a few sentences after heading matches for more context,
    but stops if another category heading is encountered.
    """
    text = str(text).strip()
    result = {cat: [] for cat in CATEGORIES}
    similarity_scores = {cat: 0.0 for cat in CATEGORIES}
    
    # === Heading-based extraction (fuzzy match via aliases) ===
    for cat, aliases in CATEGORY_ALIASES.items():
        found_match = False
        for alias in aliases:
            pattern = rf"(?i){re.escape(alias)}\s*[:\-]?\s*(.*?)(?=\n[A-Z].*[:\-]|\Z)"
            match = re.search(pattern, text, re.S)
            if match:
                value = match.group(1).strip()
                if value:
                    extracted = [v.strip() for v in value.split("\n") if v.strip()]
                    
                    # === Add extra sentences after heading block ===
                    end_pos = match.end(1)
                    remaining_text = text[end_pos:].strip()
                    
                    stop_pattern = rf"(?i)^\s*({'|'.join(re.escape(a) for aliases in CATEGORY_ALIASES.values() for a in aliases)})\s*[:\-]"
                    remaining_text = re.split(stop_pattern, remaining_text, maxsplit=1)[0]
                    
                    extra_sent_list = re.split(r'(?<=[.!?])\s+', remaining_text)
                    extra_sent_list = [s.strip() for s in extra_sent_list if s.strip()]
                    if extra_sent_list:
                        extracted.extend(extra_sent_list[:extra_sentences])
                    # =================================================
                    
                    result[cat] = extracted
                    similarity_scores[cat] = 1.0
                    found_match = True
                    break
        if found_match:
            continue
    
    # === Fallback: cosine similarity (top-N sentences) ===
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]  # preserve context
    if paragraphs:
        para_embeddings = embedder.encode(paragraphs, convert_to_tensor=True)
        
        for cat in CATEGORIES:
            if not result[cat]:  # no heading match
                cat_embedding = embedder.encode(cat, convert_to_tensor=True)
                cosine_scores = util.cos_sim(cat_embedding, para_embeddings)[0]
                
                top_indices = np.argsort(cosine_scores.cpu().numpy())[::-1][:top_k]
                top_matches = [
                    paragraphs[idx] for idx in top_indices 
                    if cosine_scores[idx] >= threshold
                ]
                
                if top_matches:
                    result[cat] = top_matches
                    similarity_scores[cat] = float(max(cosine_scores[idx] for idx in top_indices))
    
    return result, similarity_scores


# === Apply extraction for "context" column (DistilBERT) ===
MODEL_NAME = "distilbert-base-uncased"
df_results = []
df_scores = []
avg_scores = []

for context_text in df["context"]:
    extracted, scores = extract_with_best_similarity(context_text)
    df_results.append(extracted)
    df_scores.append(scores)
    avg_scores.append(np.mean(list(scores.values())) if scores else 0.0)

df[MODEL_NAME] = df_results
df["cosine_similarity_scores"] = df_scores
df["average_cosine_similarity"] = avg_scores


# === Apply cosine similarity for "gpt4" column ===
GPT4_MODEL_NAME = "gpt4_cosine_similarity_scores"
gpt4_scores = []
gpt4_avg_scores = []

for gpt4_text in df["gpt4"]:
    _, scores = extract_with_best_similarity(gpt4_text)
    gpt4_scores.append(scores)
    gpt4_avg_scores.append(np.mean(list(scores.values())) if scores else 0.0)

df[GPT4_MODEL_NAME] = gpt4_scores
df["gpt4_average_cosine_similarity"] = gpt4_avg_scores


# === Save ===
df.to_csv("results_with_best_cosine_similarity.csv", index=False)
print("âœ… Extraction + cosine similarity for 'context' and 'gpt4' columns completed and saved.")

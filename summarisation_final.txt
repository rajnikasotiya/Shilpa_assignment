import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, logging
from evaluate import load

# Suppress HF warnings
logging.set_verbosity_error()

# Load evaluation metrics
rouge = load("rouge")
bertscore = load("bertscore")
sacrebleu = load("sacrebleu")

# List of Hugging Face models to evaluate
model_names = [
    "Falconsai/medical_summarization",
    "umeshramya/t5_small_medical_512",
    "facebook/bart-large-cnn"
]

# Load and prepare dataset
def load_data(path):
    df = pd.read_csv(path)
    df = df.dropna(subset=["article", "abstract"])
    df = df.sample(5, random_state=42)  # Reduce size for faster execution
    return df

# Generate summary
def generate_summary(model, tokenizer, text, max_input_len=256, max_target_len=64):
    inputs = tokenizer(text, return_tensors="pt", max_length=max_input_len, truncation=True)
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_length=max_target_len,
            num_beams=1,
            do_sample=False
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Evaluate predictions
def evaluate_summaries(references, predictions):
    scores = {}

    # SacreBLEU requires references as List[List[str]]
    references_for_bleu = [[ref] for ref in references]

    # ROUGE
    rouge_scores = rouge.compute(predictions=predictions, references=references)
    scores["rougeL"] = round(rouge_scores["rougeL"], 4)

    # BERTScore
    bert_scores = bertscore.compute(predictions=predictions, references=references, lang="en")
    scores["bertscore_f1"] = round(sum(bert_scores["f1"]) / len(bert_scores["f1"]), 4)

    # BLEU
    bleu_scores = sacrebleu.compute(predictions=predictions, references=references_for_bleu)
    scores["bleu"] = round(bleu_scores["score"], 4)

    return scores

# Main function
def main():
    df = load_data("train.csv")  # Update path if needed

    references = df["abstract"].tolist()
    source_texts = df["article"].tolist()
    results = {}

    for model_name in model_names:
        print(f"\nüîç Evaluating model: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        predictions = []
        for text in source_texts:
            try:
                summary = generate_summary(model, tokenizer, text)
            except Exception as e:
                print(f"‚ö†Ô∏è Error in summarizing: {e}")
                summary = ""
            predictions.append(summary)

        metrics = evaluate_summaries(references, predictions)
        results[model_name] = metrics

        print(f"‚úÖ Results for {model_name}: {metrics}")

    # Final comparison
    print("\nüìä Final Model Comparison:")
    best_model = max(results.items(), key=lambda x: x[1]["rougeL"])
    for model, scores in results.items():
        print(f"{model}: {scores}")
    print(f"\nüèÜ Best model based on ROUGE-L: {best_model[0]} with score {best_model[1]}")

if __name__ == "__main__":
    import torch  # Needed for generate() to run
    main()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_log_error, make_scorer

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor

df = pd.read_csv("train.csv")  # Replace with your dataset path if different
print("Columns:\n", df.columns.tolist())
df.head()

df.info()
df.describe()
df.isna().sum()

# Visualize target variable
sns.histplot(df['Premium Amount'], kde=True)
plt.title('Premium Amount Distribution')

# Log-transform skewed income
df['log_income'] = np.log1p(df['Annual Income'])

# Ratio and interaction features
df['dependent_ratio'] = df['Number of Dependents'] / (df['Insurance Duration'] + 1)
df['age_duration_interaction'] = df['Age'] * df['Insurance Duration']


# Define features & target
features = ['Age', 'log_income', 'dependent_ratio', 'age_duration_interaction',
            'Education Level', 'Occupation', 'Health Score', 'Policy Type',
            'Previous Claims', 'Vehicle Age', 'Credit Score', 'Exercise Frequency',
            'Smoking Status', 'Property Type']

X = df[features]
y = df['Premium Amount']

# Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify column types
num_cols = X.select_dtypes(include=np.number).columns.tolist()
cat_cols = X.select_dtypes(include='object').columns.tolist()

# Preprocessor
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(drop='first'), cat_cols)
])


# Define models with hyperparameters
models = {
    'XGBoost': (XGBRegressor(random_state=42), {
        'model__n_estimators': [100, 200],
        'model__learning_rate': [0.05, 0.1],
        'model__max_depth': [3, 5]
    }),
    'Random Forest': (RandomForestRegressor(random_state=42), {
        'model__n_estimators': [100, 200],
        'model__max_depth': [None, 10],
        'model__min_samples_split': [2, 5]
    }),
    'Gradient Boosting': (GradientBoostingRegressor(random_state=42), {
        'model__n_estimators': [100, 200],
        'model__learning_rate': [0.05, 0.1],
        'model__max_depth': [3, 5]
    })
}

cv = KFold(n_splits=5, shuffle=True, random_state=42)
msle = make_scorer(mean_squared_log_error, greater_is_better=False)

# Grid search on each model
best_models = {}
for name, (model, params) in models.items():
    pipe = Pipeline([
        ('pre', preprocessor),
        ('model', model)
    ])
    grid = GridSearchCV(pipe, param_grid=params, scoring=msle, cv=cv, verbose=0, n_jobs=-1)
    grid.fit(X_train, y_train)
    best_models[name] = grid
    print(f"{name} RMSLE: {np.sqrt(-grid.best_score_):.5f}")


for name, model in best_models.items():
    y_pred = model.predict(X_test)
    rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))
    print(f"{name} Test RMSLE: {rmsle:.5f}")


# Find best based on test RMSLE
results = {name: np.sqrt(mean_squared_log_error(y_test, model.predict(X_test)))
           for name, model in best_models.items()}

best_model_name = min(results, key=results.get)
print(f"\nüèÜ Best model: {best_model_name} with Test RMSLE = {results[best_model_name]:.5f}")
